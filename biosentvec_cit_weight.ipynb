{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "biosentvec_cit_weight.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Toluwase/Citation-Weights-based-on-Citation-Contexts-Semantic-Similarity/blob/main/biosentvec_cit_weight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbgwZWWfWpp"
      },
      "source": [
        "# Python packages needed before running the Python codes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These packages were installed on a Google colab jupyter notebook. The syntax on unix or linux may be different."
      ],
      "metadata": {
        "id": "_yk64vJYQCcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Instal wget (not required)"
      ],
      "metadata": {
        "id": "OG5OU3gYP5ZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "id": "blrcuDvsOFYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Install fastText"
      ],
      "metadata": {
        "id": "nZlqc-V3OfP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "!cd fastText\n",
        "!pip install fastText"
      ],
      "metadata": {
        "id": "U4oZnT9aOgND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Install SentVec\n",
        "(Please note: if you install sent2vec using \n",
        "```\n",
        "$!pip install sent2vec\n",
        "```\n",
        "then you'll get the wrong package. Please follow the instructions in the official [Github repository](https://github.com/epfml/sent2vec) to install it correctly. Alternatively, you can use the code below. I wasted many hours trying to figure out the problem during installation because of an error: ```\n",
        "make: *** No targets specified and no makefile found. Stop.\n",
        "```"
      ],
      "metadata": {
        "id": "eIbCzH8URKHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/epfml/sent2vec/archive/master.zip\n",
        "!unzip master.zip\n",
        "!make\n",
        "!sudo pip install sent2vec-master/"
      ],
      "metadata": {
        "id": "exI1MGFVPIMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Python script"
      ],
      "metadata": {
        "id": "VaKZMLpMStJo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAb77yZ9fzMG"
      },
      "source": [
        "import sent2vec\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from scipy.spatial import distance\n",
        "\n",
        "model_path = ('/content/drive/MyDrive/Colab Notebooks/BioSentVec_PubMed_MIMICIII-bigram_d700.bin')\n",
        "model = sent2vec.Sent2vecModel()\n",
        "try:\n",
        "    model.load_model(model_path)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "print('model successfully loaded')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess_sentence(text):\n",
        "    text = text.replace('/', ' / ')\n",
        "    text = text.replace('.-', ' .- ')\n",
        "    text = text.replace('.', ' . ')\n",
        "    text = text.replace('\\'', ' \\' ')\n",
        "    text = text.lower()\n",
        "\n",
        "    tokens = [token for token in word_tokenize(text) if token not in punctuation and token not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/direct_input/direct_input_1.txt\") as my_file:\n",
        "  lines=my_file.readlines()\n",
        "#print (lines)\n",
        "s1 = []\n",
        "s2 = []\n",
        "score = []\n",
        "y=0\n",
        "for everyline in lines:\n",
        "    if y<len(lines)-2:\n",
        "        sentenceONE=preprocess_sentence(lines[y])\n",
        "        sentenceTWO=preprocess_sentence(lines[y+1])\n",
        "        sentenceONE_vector = model.embed_sentence(sentenceONE)\n",
        "        sentenceTWO_vector = model.embed_sentence(sentenceTWO)\n",
        "        cosine_sim = 1 - distance.cosine(sentenceONE_vector, sentenceTWO_vector)\n",
        "        print(lines[y],\"\\t\", lines[y+1], \"\\t\", 'cosine similarity:', cosine_sim)\n",
        "        s1.append(lines[y])\n",
        "        s2.append(lines[y+1])\n",
        "        score.append(str(cosine_sim))\n",
        "    y=y+3\n",
        "    \n",
        "#with open('output.txt', \"w\", encoding=\"utf-8\") as w_f:\n",
        "#    for i in range(len(s1)):\n",
        "#        w_f.write(s1[i])\n",
        "#        w_f.write('\\t')\n",
        "#        w_f.write(s2[i])\n",
        "#        w_f.write('\\t')\n",
        "#        w_f.write(score[i])\n",
        "#        w_f.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pafL7Li0jyXW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}