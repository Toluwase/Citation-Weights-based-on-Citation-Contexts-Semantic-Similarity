{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "biosentvec_cit_weight.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Toluwase/Citation-Weights-based-on-Citation-Contexts-Semantic-Similarity/blob/main/biosentvec_cit_weight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbgwZWWfWpp"
      },
      "source": [
        "# Python packages needed before running the Python codes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These packages were installed on a Google colab jupyter notebook. The syntax on unix or linux may be different."
      ],
      "metadata": {
        "id": "_yk64vJYQCcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Instal wget (not required)"
      ],
      "metadata": {
        "id": "OG5OU3gYP5ZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "id": "blrcuDvsOFYQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5b3a9c9-2062-41b0-9a7b-2870b045aa69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=379211408d45db0d2b820eadccf3a40cc4f2422c33043e3de93b5d8fd6a1f09c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Install fastText"
      ],
      "metadata": {
        "id": "nZlqc-V3OfP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "!cd fastText\n",
        "!pip install fastText"
      ],
      "metadata": {
        "id": "U4oZnT9aOgND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b205d06-10a0-4531-eff2-4a7aee74d788"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'fastText' already exists and is not an empty directory.\n",
            "Requirement already satisfied: fastText in /usr/local/lib/python3.7/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fastText) (2.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastText) (1.21.5)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fastText) (57.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Install SentVec\n",
        "(Please note: if you install sent2vec using \n",
        "```\n",
        "$!pip install sent2vec\n",
        "```\n",
        "then you'll get the wrong package. Please follow the instructions in the official [Github repository](https://github.com/epfml/sent2vec) to install it correctly. Alternatively, you can use the code below. I wasted many hours trying to figure out the problem during installation because of an error: ```\n",
        "make: *** No targets specified and no makefile found. Stop.\n",
        "```"
      ],
      "metadata": {
        "id": "eIbCzH8URKHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/epfml/sent2vec/archive/master.zip\n",
        "!unzip master.zip\n",
        "!make\n",
        "!sudo pip install sent2vec-master/"
      ],
      "metadata": {
        "id": "exI1MGFVPIMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d1dd78-4235-4fbd-b272-3cc19a84c8fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-17 06:16:00--  https://github.com/epfml/sent2vec/archive/master.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/epfml/sent2vec/zip/refs/heads/master [following]\n",
            "--2022-03-17 06:16:01--  https://codeload.github.com/epfml/sent2vec/zip/refs/heads/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.114.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.114.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip.1’\n",
            "\n",
            "master.zip.1            [ <=>                ] 356.24K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-03-17 06:16:01 (2.98 MB/s) - ‘master.zip.1’ saved [364785]\n",
            "\n",
            "Archive:  master.zip\n",
            "770bd2d475c35eccc9a2452592e7c4304ac89fb9\n",
            "replace sent2vec-master/.gitignore? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/.gitignore  \n",
            "replace sent2vec-master/Dockerfile? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/Dockerfile  \n",
            "replace sent2vec-master/LICENSE? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/LICENSE  \n",
            "replace sent2vec-master/Makefile? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/Makefile  \n",
            "replace sent2vec-master/README.md? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/README.md  \n",
            "replace sent2vec-master/get_sentence_embeddings_from_pre-trained_models.ipynb? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/get_sentence_embeddings_from_pre-trained_models.ipynb  \n",
            "replace sent2vec-master/paper-sent2vec.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/paper-sent2vec.pdf  \n",
            "replace sent2vec-master/pyproject.toml? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/pyproject.toml  \n",
            "replace sent2vec-master/requirements.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            " extracting: sent2vec-master/requirements.txt  \n",
            "replace sent2vec-master/setup.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace sent2vec-master/setup.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/setup.py  \n",
            "replace sent2vec-master/src/args.cc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/args.cc  \n",
            "replace sent2vec-master/src/args.h? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/args.h  \n",
            "replace sent2vec-master/src/asvoid.h? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/asvoid.h  \n",
            "replace sent2vec-master/src/dictionary.cc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/dictionary.cc  \n",
            "replace sent2vec-master/src/dictionary.h? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/dictionary.h  \n",
            "replace sent2vec-master/src/fasttext.cc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/fasttext.cc  \n",
            "replace sent2vec-master/src/fasttext.h? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/fasttext.h  \n",
            "replace sent2vec-master/src/main.cc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/main.cc  \n",
            "replace sent2vec-master/src/matrix.cc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/matrix.cc  \n",
            "replace sent2vec-master/src/matrix.h? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/matrix.h  \n",
            "replace sent2vec-master/src/model.cc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/model.cc  \n",
            "replace sent2vec-master/src/model.h? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/model.h  \n",
            "replace sent2vec-master/src/productquantizer.cc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/productquantizer.cc  \n",
            "replace sent2vec-master/src/productquantizer.h? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/productquantizer.h  \n",
            "replace sent2vec-master/src/qmatrix.cc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/qmatrix.cc  \n",
            "replace sent2vec-master/src/qmatrix.h? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/qmatrix.h  \n",
            "replace sent2vec-master/src/real.cc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            " extracting: sent2vec-master/src/real.cc  \n",
            "replace sent2vec-master/src/real.h? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/real.h  \n",
            "replace sent2vec-master/src/sent2vec.pyx? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/sent2vec.pyx  \n",
            "replace sent2vec-master/src/shmem_matrix.cc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/shmem_matrix.cc  \n",
            "replace sent2vec-master/src/shmem_matrix.h? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/shmem_matrix.h  \n",
            "replace sent2vec-master/src/utils.cc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/utils.cc  \n",
            "replace sent2vec-master/src/utils.h? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/utils.h  \n",
            "replace sent2vec-master/src/vector.cc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/vector.cc  \n",
            "replace sent2vec-master/src/vector.h? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/src/vector.h  \n",
            "replace sent2vec-master/tweetTokenize.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/tweetTokenize.py  \n",
            "replace sent2vec-master/wikiTokenize.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sent2vec-master/wikiTokenize.py  \n",
            "make: *** No targets specified and no makefile found.  Stop.\n",
            "Processing ./sent2vec-master\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Cython>=0.29.13 in /usr/local/lib/python3.7/dist-packages (from sent2vec==0.0.0) (0.29.28)\n",
            "Requirement already satisfied: numpy>=1.17.1 in /usr/local/lib/python3.7/dist-packages (from sent2vec==0.0.0) (1.21.5)\n",
            "Building wheels for collected packages: sent2vec\n",
            "  Building wheel for sent2vec (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sent2vec: filename=sent2vec-0.0.0-cp37-cp37m-linux_x86_64.whl size=1129228 sha256=3b5872c1129ee47a367f5b83a5bd8ff36353660c577685e1cc7feae7cab0d85f\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/c5/86/43698c9ffe2a27cf0157acb724fde5a1db0ba9da14a1fae745\n",
            "Successfully built sent2vec\n",
            "Installing collected packages: sent2vec\n",
            "  Attempting uninstall: sent2vec\n",
            "    Found existing installation: sent2vec 0.0.0\n",
            "    Uninstalling sent2vec-0.0.0:\n",
            "      Successfully uninstalled sent2vec-0.0.0\n",
            "Successfully installed sent2vec-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKHhxHutd0kH",
        "outputId": "70d907ca-56bc-4692-8e68-c821fbda9c85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM5gLlvxiNoN",
        "outputId": "3d585c01-3b05-4d94-ae48-eb85d0b1dda7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/extended_omw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Python script"
      ],
      "metadata": {
        "id": "VaKZMLpMStJo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAb77yZ9fzMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013f0498-9bdc-4e77-ef19-5dfac9e0de18"
      },
      "source": [
        "import sent2vec\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from scipy.spatial import distance\n",
        "\n",
        "model_path = ('/content/drive/MyDrive/Colab Notebooks/BioSentVec_PubMed_MIMICIII-bigram_d700.bin')\n",
        "model = sent2vec.Sent2vecModel()\n",
        "try:\n",
        "    model.load_model(model_path)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "print('model successfully loaded')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess_sentence(text):\n",
        "    text = text.replace('/', ' / ')\n",
        "    text = text.replace('.-', ' .- ')\n",
        "    text = text.replace('.', ' . ')\n",
        "    text = text.replace('\\'', ' \\' ')\n",
        "    text = text.lower()\n",
        "\n",
        "    tokens = [token for token in word_tokenize(text) if token not in punctuation and token not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/direct_input/direct_input_100.txt\") as my_file:\n",
        "  lines=my_file.readlines()\n",
        "#print (lines)\n",
        "s1 = []\n",
        "s2 = []\n",
        "score = []\n",
        "y=0\n",
        "for everyline in lines:\n",
        "    if y<len(lines)-2:\n",
        "        sentenceONE=preprocess_sentence(lines[y])\n",
        "        sentenceTWO=preprocess_sentence(lines[y+1])\n",
        "        sentenceONE_vector = model.embed_sentence(sentenceONE)\n",
        "        sentenceTWO_vector = model.embed_sentence(sentenceTWO)\n",
        "        cosine_sim = 1 - distance.cosine(sentenceONE_vector, sentenceTWO_vector)\n",
        "        print(lines[y],\"\\t\", lines[y+1], \"\\t\", 'cosine similarity:', cosine_sim)\n",
        "        s1.append(lines[y])\n",
        "        s2.append(lines[y+1])\n",
        "        score.append(str(cosine_sim))\n",
        "    y=y+3\n",
        "    \n",
        "#with open('output.txt', \"w\", encoding=\"utf-8\") as w_f:\n",
        "#    for i in range(len(s1)):\n",
        "#        w_f.write(s1[i])\n",
        "#        w_f.write('\\t')\n",
        "#        w_f.write(s2[i])\n",
        "#        w_f.write('\\t')\n",
        "#        w_f.write(score[i])\n",
        "#        w_f.write('\\n')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model successfully loaded\n",
            "and in 41 studies in which CBA-IF was used , 29 % were MOG-Ab-positive\n",
            " \t Summary data from 61 studies reveal the association of MOG-Abs with non-multiple sclerosis inflammatory demyelinating CNS diseases by age , and demonstrates that the proportion of individuals with an acquired demyelinating disease who are positive for MOG-Abs is greater among children ( 40 % ) than among mixed cohorts ( 29 % ) and adults ( 22 % ) . Most of these studies included selected populations , and cell-based assays ( CBA-FACS or CBA-IF ) were used to analyse MOG-Abs in children ( 18 studies )\n",
            " \t cosine similarity: 0.2460452914237976\n",
            "In this review , Table 5 summarizes all of the cases of NMOSD associated with encephalitis ( n = 46 ) reported in the literature from 2010 onward\n",
            " \t While the authors of these case reports or series aimed to report clinical cases with “ overlap syndrome ” , the series by in_text_ref . was a systemic evaluation of CNS autoantibodies in pediatric demyelination syndrome including ADEM , optic neuritis , LETM , and neuromyelitis optica\n",
            " \t cosine similarity: 0.6112399101257324\n",
            "In this review , Table 5 summarizes all of the cases of NMOSD associated with encephalitis ( n = 46 ) reported in the literature from 2010 onward\n",
            " \t Serum NMDA titers were not available for the majority of cases , except in one study . Glycine receptor and voltage-gated potassium channel antibodies were positive in one and three cases , respectively\n",
            " \t cosine similarity: 0.4033065140247345\n",
            "While the authors of these case reports or series aimed to report clinical cases with “ overlap syndrome ” , the series by in_text_ref . was a systemic evaluation of CNS autoantibodies in pediatric demyelination syndrome including ADEM , optic neuritis , LETM , and neuromyelitis optica\n",
            " \t Serum NMDA titers were not available for the majority of cases , except in one study . Glycine receptor and voltage-gated potassium channel antibodies were positive in one and three cases , respectively\n",
            " \t cosine similarity: 0.4422975480556488\n",
            "However , patients who were PNMA2/Ta and Yo antibody positive presented with classical symptoms and MRI findings for demyelinating syndromes . The follow-up of these patients was longer than the average follow-up intervals reported in a previous study\n",
            " \t However , serum NMDAR antibody positivity in patients with demyelinating syndrome without encephalopathy has previously been reported , in line with our finding\n",
            " \t cosine similarity: 0.5476340651512146\n",
            "Since the identification of aquaporin-4 ( AQP4 ) antibody in neuromyelitis optica , recent studies have also reported the presence of other autoantibodies in acquired demyelinating syndrome , including Pediatric acute transverse myelitis\n",
            " \t Pediatric acute transverse myelitis may have a relapsing course , which could be categorized as relapsing Pediatric acute transverse myelitis a presentation of multiple sclerosis , part of a systemic autoimmune disease , or NMOSD in the setting of identified antibodies\n",
            " \t cosine similarity: 0.7436860799789429\n",
            "The incidence of NMDAR‐Ab in this cohort of patients with brainstem encephalitis ( 12 % ) but higher than an unselected cohort of children with encephalitis from Australia and a UK cohort of children presenting with acquired demyelinating syndromes in whom NMDAR‐Ab were identified in only 6 % and 3 % respectively\n",
            " \t similarly , in this cohort , where patients with acquired demyelinating syndromes also had either clinical or radiological evidence of brainstem involvement , we identified a higher incidence of neuronal and glial antibodies ( 58 % ) compared with a previous study of antibody prevalence in a range of childhood demyelination syndromes , where 4 of 14 ( 29 % ) ADEM patients had glial ( MOG ) and neuronal ( anti‐VGKC‐complex and anti‐NMDAR ) antibodies\n",
            " \t cosine similarity: 0.7165358066558838\n",
            "Following studies confirmed and extended these previous findings\n",
            " \t Screening different patient groups identified MOG antibody positive , AQP4 antibody negative patients with neuromyelitis optica and related disorders\n",
            " \t cosine similarity: 0.1484154611825943\n",
            "A cell-based assay can be successfully used to detect anti-MOG immunoglogulin ( IgG ) in sera samples from pediatric patients with acute disseminated encephalomyelitis\n",
            " \t A cell-based assay can be successfully used to detect anti-MOG immunoglogulin ( IgG ) in sera samples from pediatric patients and also in adults and pediatric patients with transverse myelitis\n",
            " \t cosine similarity: 0.8780125379562378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9y_Yf9QfdybR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pafL7Li0jyXW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}